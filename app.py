import logging
import os
import sys
import asyncio
from pathlib import Path

# Fix pour Windows: utiliser SelectorEventLoop au lieu de ProactorEventLoop
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# Load environment variables FIRST, before any imports that depend on them
from dotenv import load_dotenv
load_dotenv()

sys.path.append(str(Path(__file__).parent.parent))
logging.basicConfig(level=logging.ERROR, format="%(message)s")
import json
from contextlib import asynccontextmanager
from uuid import uuid4

import uvicorn
from fastapi import Depends, FastAPI, HTTPException, Request, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from langchain.schema import Document, HumanMessage
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pydantic import BaseModel
from tavily import TavilyClient
import psycopg2
import numpy as np
from openai import OpenAI
from datetime import datetime

from crag_graph import get_crag_graph

# Configuration PostgreSQL pour PGVector uniquement
postgres_connection_string = os.getenv("POSTGRES_CONNECTION_STRING")


app = FastAPI(title="Dagan Agent RAG API", version="2.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    return {"status": "everything is ok"}


class VectorizeRequest(BaseModel):
    url: str
    # Pas de thread_id nÃ©cessaire : documents publics partagÃ©s


class CragQueryRequest(BaseModel):
    question: str
    conversation_id: str = None  # Optional, sera gÃ©nÃ©rÃ© si non fourni


@app.get("/")
async def ping():
    return {"message": "Alive"}


@app.post("/vectorize-file")
async def vectorize_file(
    file: UploadFile = File(...),
    collection_name: str = Form(None)
):
    """
    Vectorise le contenu d'un fichier texte (.txt) en chunks avec embeddings.
    
    Args:
        file: Fichier .txt Ã  vectoriser
        collection_name: Nom de la collection (dÃ©faut: "file_uploads")
        
    Returns:
        JSON avec rÃ©sumÃ© de la vectorisation
        
    Limites:
        - Taille max: 10 MB
        - Format: .txt uniquement
        
    Process:
        1. Validation fichier (extension, taille)
        2. Lecture contenu texte
        3. Chunking avec overlap (4000 chars, 800 overlap)
        4. GÃ©nÃ©ration embeddings OpenAI
        5. Stockage dans PGVector avec mÃ©tadonnÃ©es
    """
    try:
        # 1. Validation de l'extension
        if not file.filename.endswith('.txt'):
            raise HTTPException(
                status_code=400,
                detail="Format de fichier non supportÃ©. Uniquement .txt acceptÃ©."
            )
        
        # 2. Lecture du contenu
        content = await file.read()
        
        # 3. Validation de la taille (10 MB max)
        max_size = 10 * 1024 * 1024  # 10 MB
        file_size = len(content)
        
        if file_size > max_size:
            raise HTTPException(
                status_code=400,
                detail=f"Fichier trop volumineux ({file_size} bytes). Maximum: {max_size} bytes (10 MB)."
            )
        
        # 4. DÃ©codage du contenu en texte
        try:
            text_content = content.decode('utf-8')
        except UnicodeDecodeError:
            # Essayer avec d'autres encodages
            try:
                text_content = content.decode('latin-1')
            except Exception as e:
                raise HTTPException(
                    status_code=400,
                    detail="Impossible de dÃ©coder le fichier. Assurez-vous qu'il s'agit d'un fichier texte valide (UTF-8 ou Latin-1)."
                )
        
        if not text_content.strip():
            raise HTTPException(
                status_code=400,
                detail="Le fichier est vide ou ne contient pas de texte valide."
            )
        
        print(f"âœ“ Fichier '{file.filename}' lu avec succÃ¨s ({file_size} bytes, {len(text_content)} caractÃ¨res)")
        
        # 5. Chunking avec overlap (mÃªme stratÃ©gie que /vectorize)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=4000,
            chunk_overlap=800,
            separators=["\n\n", "\n", ".", " ", ""],
            length_function=len
        )
        
        chunks = text_splitter.split_text(text_content)
        
        if not chunks:
            raise HTTPException(
                status_code=400,
                detail="Aucun chunk gÃ©nÃ©rÃ©. Le contenu est peut-Ãªtre trop court."
            )
        
        print(f"âœ“ {len(chunks)} chunks gÃ©nÃ©rÃ©s avec overlap (size=4000, overlap=800)")
        
        # 6. PrÃ©parer les mÃ©tadonnÃ©es
        collection = collection_name or "file_uploads"
        upload_timestamp = datetime.utcnow().isoformat()
        
        documents = []
        for chunk_index, chunk_content in enumerate(chunks):
            doc = Document(
                page_content=chunk_content,
                metadata={
                    "filename": file.filename,
                    "file_size": file_size,
                    "upload_date": upload_timestamp,
                    "file_type": "text/plain",
                    "source": "file_upload",
                    "chunk_index": chunk_index,
                    "chunk_count": len(chunks),
                    "chunk_size": len(chunk_content)
                }
            )
            documents.append(doc)
        
        print(f"âœ“ {len(documents)} documents crÃ©Ã©s avec mÃ©tadonnÃ©es")
        
        # 7. GÃ©nÃ©ration des embeddings et stockage dans PGVector
        openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        conn = psycopg2.connect(postgres_connection_string)
        cursor = conn.cursor()
        
        # VÃ©rifier/crÃ©er la table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS langchain_pg_embedding (
                id TEXT PRIMARY KEY,
                collection_id TEXT,
                embedding VECTOR(2000),
                document TEXT,
                cmetadata JSONB
            )
        """)
        
        # CrÃ©er l'index si nÃ©cessaire
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS langchain_pg_embedding_embedding_idx 
            ON langchain_pg_embedding 
            USING ivfflat (embedding vector_cosine_ops)
            WITH (lists = 100)
        """)
        
        conn.commit()
        
        # 8. GÃ©nÃ©rer embeddings et insÃ©rer
        uuids = [str(uuid4()) for _ in range(len(documents))]
        
        for i, (doc, doc_id) in enumerate(zip(documents, uuids)):
            # GÃ©nÃ©rer embedding
            response = openai_client.embeddings.create(
                model="text-embedding-3-large",
                input=doc.page_content,
                dimensions=2000
            )
            embedding = response.data[0].embedding
            
            # InsÃ©rer dans PGVector
            cursor.execute("""
                INSERT INTO langchain_pg_embedding (id, collection_id, embedding, document, cmetadata)
                VALUES (%s, %s, %s::vector, %s, %s)
                ON CONFLICT (id) DO UPDATE 
                SET embedding = EXCLUDED.embedding, 
                    document = EXCLUDED.document, 
                    cmetadata = EXCLUDED.cmetadata
            """, (doc_id, collection, embedding, doc.page_content, json.dumps(doc.metadata)))
        
        conn.commit()
        cursor.close()
        conn.close()
        
        print(f"âœ“ {len(documents)} documents vectorisÃ©s et stockÃ©s dans collection '{collection}'")
        
        return JSONResponse(
            content={
                "success": True,
                "message": f"Fichier '{file.filename}' vectorisÃ© avec succÃ¨s",
                "filename": file.filename,
                "file_size": file_size,
                "collection": collection,
                "documents_count": len(documents),
                "chunks_info": {
                    "chunk_size": 4000,
                    "chunk_overlap": 800,
                    "total_chunks": len(chunks),
                    "total_characters": len(text_content)
                },
                "upload_date": upload_timestamp
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de la vectorisation du fichier: {str(e)}"
        )

@app.post("/vectorize")
async def vectorize_url(
    body: VectorizeRequest,
):
    """
    Vectorize a URL by crawling it and creating embeddings in PostgreSQL
    with intelligent chunking (overlap between chunks for better context).
    
    Process:
    1. Crawl URL with Tavily (get raw content + favicon)
    2. Split content into chunks with overlap (RecursiveCharacterTextSplitter)
    3. Vectorize each chunk with OpenAI embeddings (direct API)
    4. Store in PGVector with metadata (url, favicon, chunk_index, chunk_count)
    """
    try:
        tavily_api_key = os.getenv("TAVILY_API_KEY")
        tavily_client = TavilyClient(api_key=tavily_api_key)
        
        # 1. Crawl URL
        crawl_result = tavily_client.crawl(
            url=body.url, format="text", include_favicon=True, limit=4
        )

        # 2. Combine all content from Tavily results
        combined_content = ""
        url_favicon_map = {}
        
        for result in crawl_result["results"]:
            raw_content = result.get("raw_content")
            url = result.get("url", "")
            favicon = result.get("favicon", "")
            
            if raw_content:
                combined_content += raw_content + "\n\n"
                url_favicon_map[url] = favicon
        
        if not combined_content.strip():
            raise HTTPException(status_code=400, detail="No content found to vectorize")
        
        # 3. Split content into chunks with overlap
        # chunk_size=1000 tokens â‰ˆ 4000 characters
        # overlap=200 tokens â‰ˆ 800 characters
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=4000,           # ~1000 tokens
            chunk_overlap=800,         # ~200 tokens overlap
            separators=["\n\n", "\n", ".", " ", ""],  # Smart separators
            length_function=len
        )
        
        chunks = text_splitter.split_text(combined_content)
        
        if not chunks:
            raise HTTPException(status_code=400, detail="No chunks generated from content")
        
        print(f"âœ“ {len(chunks)} chunks gÃ©nÃ©rÃ©s avec overlap (size=4000, overlap=800)")
        
        # 4. Create documents from chunks with metadata
        documents = []
        primary_url = body.url
        primary_favicon = url_favicon_map.get(primary_url, "")
        
        for chunk_index, chunk_content in enumerate(chunks):
            doc = Document(
                page_content=chunk_content,
                metadata={
                    "url": primary_url,
                    "favicon": primary_favicon,
                    "chunk_index": chunk_index,
                    "chunk_count": len(chunks),
                    "chunk_size": len(chunk_content)
                }
            )
            documents.append(doc)
        
        print(f"âœ“ {len(documents)} documents crÃ©Ã©s avec mÃ©tadonnÃ©es de chunks")

        # 5. Initialize OpenAI client for embeddings (direct API)
        openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # 6. Connect to PostgreSQL and create embeddings
        conn = psycopg2.connect(postgres_connection_string)
        cursor = conn.cursor()
        
        collection_name = os.getenv("DOCUMENTS_COLLECTION", "crawled_documents")
        
        # Modifier la table existante pour changer collection_id de UUID Ã  TEXT
        # D'abord, vÃ©rifier si la table existe et si la colonne est de type UUID
        cursor.execute("""
            SELECT column_name, data_type 
            FROM information_schema.columns 
            WHERE table_name = 'langchain_pg_embedding' 
            AND column_name = 'collection_id'
        """)
        column_info = cursor.fetchone()
        
        if column_info and column_info[1] == 'uuid':
            # La colonne existe et est de type UUID, on la modifie en TEXT
            print("âš ï¸  Modification de la colonne collection_id (UUID â†’ TEXT)...")
            cursor.execute("""
                ALTER TABLE langchain_pg_embedding 
                ALTER COLUMN collection_id TYPE TEXT 
                USING collection_id::TEXT
            """)
            conn.commit()
            print("Colonne collection_id modifiÃ©e en TEXT")
        
        # Create table if not exists (avec collection_id en TEXT)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS langchain_pg_embedding (
                id TEXT PRIMARY KEY,
                collection_id TEXT,
                embedding VECTOR(2000),
                document TEXT,
                cmetadata JSONB
            )
        """)
        
        # CrÃ©er un index pour optimiser les recherches de similaritÃ© si pas existant
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS langchain_pg_embedding_embedding_idx 
            ON langchain_pg_embedding 
            USING ivfflat (embedding vector_cosine_ops)
            WITH (lists = 100)
        """)
        
        conn.commit()
        
        # 7. Generate embeddings and store in PGVector
        uuids = [str(uuid4()) for _ in range(len(documents))]
        
        for i, (doc, doc_id) in enumerate(zip(documents, uuids)):
            # Generate embedding using direct OpenAI API
            response = openai_client.embeddings.create(
                model="text-embedding-3-large",
                input=doc.page_content,
                dimensions=2000
            )
            embedding = response.data[0].embedding
            
            # Store in PGVector avec collection_name en TEXT
            cursor.execute("""
                INSERT INTO langchain_pg_embedding (id, collection_id, embedding, document, cmetadata)
                VALUES (%s, %s, %s::vector, %s, %s)
                ON CONFLICT (id) DO UPDATE 
                SET embedding = EXCLUDED.embedding, 
                    document = EXCLUDED.document, 
                    cmetadata = EXCLUDED.cmetadata
            """, (doc_id, collection_name, embedding, doc.page_content, json.dumps(doc.metadata)))
        
        conn.commit()
        cursor.close()
        conn.close()
        
        print(f"âœ“ {len(documents)} documents vectorisÃ©s et stockÃ©s dans PGVector")

        return JSONResponse(
            content={
                "success": True,
                "message": f"Successfully vectorized {len(documents)} chunks from {body.url}",
                "documents_count": len(documents),
                "chunks_info": {
                    "chunk_size": 4000,
                    "chunk_overlap": 800,
                    "total_chunks": len(chunks)
                }
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error vectorizing URL: {str(e)}")





@app.post("/crag/query")
async def crag_query(
    body: CragQueryRequest,
    fastapi_request: Request,
):
    """
    Endpoint pour tester le workflow Hybrid RAG complet avec mÃ©moire conversationnelle
    
    Le workflow Hybrid RAG:
    1. ROUTE_QUESTION: Classifie la question (casual vs admin)
    2. CASUAL_CONVO: RÃ©ponses amicales pour conversations informelles
    3. AGENT_RAG: Agent ReAct pour questions administratives
    
    Args:
        body: CragQueryRequest avec question et conversation_id optionnel
        
    Returns:
        JSON avec la rÃ©ponse gÃ©nÃ©rÃ©e et des mÃ©tadonnÃ©es du workflow
    """
    try:
        # GÃ©nÃ©rer un conversation_id si non fourni (utiliser thread_id pour LangGraph)
        thread_id = body.conversation_id or str(uuid4())
        
        print(f"\n{'='*60}")
        print(f"Hybrid RAG Query Request")
        print(f"{'='*60}")
        print(f"Question: {body.question}")
        print(f"Thread ID: {thread_id}")
        print(f"{'='*60}\n")
        
        # RÃ©cupÃ©rer le graph Agent RAG (avec InMemorySaver intÃ©grÃ©)
        agent_graph = get_crag_graph()
        
        # PrÃ©parer l'Ã©tat initial avec MessagesState
        # On crÃ©e un HumanMessage avec la question
        initial_state = {
            "messages": [HumanMessage(content=body.question)],
            "question": body.question,
            "domain_validated": False
        }
        
        # Configuration pour le checkpointer (thread_id pour la mÃ©moire)
        config = {"configurable": {"thread_id": thread_id}}
        
        # ExÃ©cuter le workflow Agent RAG avec persistance de la mÃ©moire (SYNC avec InMemorySaver)
        final_state = agent_graph.invoke(initial_state, config)
        
        print(f"\n{'='*60}")
        print(f"Hybrid RAG Workflow Completed")
        print(f"{'='*60}")
        print(f"Messages: {len(final_state.get('messages', []))}")
        print(f"{'='*60}\n")
        
        # Extraire la rÃ©ponse finale des messages
        messages = final_state.get("messages", [])
        final_answer = ""
        sources = []
        
        # Trouver le dernier AIMessage et extraire sources
        for msg in reversed(messages):
            if hasattr(msg, 'type') and msg.type == 'ai':
                final_answer = msg.content
                # Extraire les sources des additional_kwargs si prÃ©sentes
                if hasattr(msg, 'additional_kwargs'):
                    sources = msg.additional_kwargs.get("sources", [])
                break
        
        # Si pas de rÃ©ponse trouvÃ©e dans les messages, essayer l'ancien format
        if not final_answer:
            final_answer = final_state.get("response", "Aucune rÃ©ponse gÃ©nÃ©rÃ©e")
        
        response_data = {
            "success": True,
            "conversation_id": thread_id,
            "question": body.question,
            "answer": final_answer,
            "sources": sources,  # Liste complÃ¨te des sources avec URLs
            "metadata": {
                "workflow": "hybrid_rag",
                "messages_count": len(messages),
                "sources_count": len(sources)
            }
        }
        
        return JSONResponse(content=response_data)
        
    except Exception as e:
        print(f"Erreur dans Agent RAG workflow: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500, 
            detail=f"Error in Agent RAG workflow: {str(e)}"
        )


@app.post("/crag/stream")
async def crag_stream(
    body: CragQueryRequest,
    fastapi_request: Request,
):
    """
    Endpoint Hybrid RAG avec streaming en temps rÃ©el (Server-Sent Events).
    
    Version streaming de /crag/query qui permet de suivre l'exÃ©cution
    du workflow node par node et de recevoir la rÃ©ponse token par token.
    
    Le workflow Hybrid RAG:
    1. ROUTE_QUESTION: Classifie la question (casual vs admin)
    2. CASUAL_CONVO: RÃ©ponses amicales pour conversations informelles  
    3. AGENT_RAG: Agent ReAct pour questions administratives
    
    Args:
        body: CragQueryRequest avec question et conversation_id optionnel
        
    Returns:
        StreamingResponse avec events SSE (Server-Sent Events)
        
    Format des events:
        - {"type": "node_start", "node": "route_question"}
        - {"type": "node_end", "node": "route_question", "question_type": "casual|admin"}
        - {"type": "node_start", "node": "casual_convo"}
        - {"type": "message_chunk", "content": "...", "node": "casual_convo"}
        - {"type": "node_start", "node": "agent_rag"}
        - {"type": "message_chunk", "content": "...", "node": "agent_rag"}
        - {"type": "complete", "conversation_id": "...", "answer": "...", "sources": [...]}
    """
    # No Authorization header required for streaming endpoint
    
    async def event_generator():
        """GÃ©nÃ¨re les events SSE pour le streaming."""
        try:
            # GÃ©nÃ©rer un conversation_id si non fourni
            thread_id = body.conversation_id or str(uuid4())
            
            print(f"\n{'='*60}")
            print(f"Hybrid RAG Stream Request")
            print(f"{'='*60}")
            print(f"Question: {body.question}")
            print(f"Thread ID: {thread_id}")
            print(f"{'='*60}\n")
            
            # RÃ©cupÃ©rer le graph Agent RAG
            agent_graph = get_crag_graph()
            
            # PrÃ©parer l'Ã©tat initial avec MessagesState
            initial_state = {
                "messages": [HumanMessage(content=body.question)],
                "question": body.question,
                "domain_validated": False
            }
            
            # Configuration pour le checkpointer
            config = {"configurable": {"thread_id": thread_id}}
            
            # Variables pour accumuler la rÃ©ponse et les sources
            accumulated_answer = ""
            collected_sources = []
            
            # Streamer le workflow Agent RAG
            async for event in agent_graph.astream(initial_state, config):
                # event est un dict avec une clÃ© = nom du node
                # et valeur = Ã©tat retournÃ© par ce node
                
                for node_name, node_output in event.items():
                    print(f"Node: {node_name}")
                    
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    # ROUTE_QUESTION node
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if node_name == "route_question":
                        question_type = node_output.get("question_type", "admin")
                        
                        # Ã‰mettre un status pour route_question
                        yield (
                            json.dumps({
                                "type": "status",
                                "step": "route_question",
                                "message": "Classification de la question..."
                            }) + "\n"
                        )
                        
                        yield (
                            json.dumps({
                                "type": "node_start",
                                "node": "route_question",
                                "message": "Classification de la question..."
                            }) + "\n"
                        )
                        
                        yield (
                            json.dumps({
                                "type": "node_end",
                                "node": "route_question",
                                "question_type": question_type,
                                "message": f"Question classifiÃ©e: {question_type}"
                            }) + "\n"
                        )
                    
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    # CASUAL_CONVO node
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    elif node_name == "casual_convo":
                        # Ã‰mettre un status pour casual_convo
                        yield (
                            json.dumps({
                                "type": "status",
                                "step": "casual_convo",
                                "message": "Conversation informelle..."
                            }) + "\n"
                        )
                        
                        yield (
                            json.dumps({
                                "type": "node_start",
                                "node": "casual_convo",
                                "message": "GÃ©nÃ©ration de rÃ©ponse conversationnelle..."
                            }) + "\n"
                        )
                        
                        # Extraire la rÃ©ponse du dernier AIMessage
                        messages = node_output.get("messages", [])
                        
                        for msg in reversed(messages):
                            if hasattr(msg, 'type') and msg.type == 'ai':
                                accumulated_answer = msg.content
                                break
                        
                        # Streaming caractÃ¨re par caractÃ¨re pour rÃ©ponses casual
                        for char in accumulated_answer:
                            yield (
                                json.dumps({
                                    "type": "message_chunk",
                                    "content": char,
                                    "node": "casual_convo"
                                }) + "\n"
                            )
                            await asyncio.sleep(0.005)  # 5ms entre chaque caractÃ¨re
                        
                        yield (
                            json.dumps({
                                "type": "node_end",
                                "node": "casual_convo",
                                "message": f"RÃ©ponse conversationnelle gÃ©nÃ©rÃ©e ({len(accumulated_answer)} caractÃ¨res)"
                            }) + "\n"
                        )
                    
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    # AGENT_RAG node
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    elif node_name == "agent_rag":
                        # Ã‰mettre un status pour agent_rag
                        yield (
                            json.dumps({
                                "type": "status",
                                "step": "agent_rag",
                                "message": "Agent ReAct en cours..."
                            }) + "\n"
                        )
                        
                        yield (
                            json.dumps({
                                "type": "node_start",
                                "node": "agent_rag",
                                "message": "Agent ReAct en cours d'exÃ©cution..."
                            }) + "\n"
                        )
                        
                        # Extraire la rÃ©ponse et les sources du dernier AIMessage
                        messages = node_output.get("messages", [])
                        
                        for msg in reversed(messages):
                            if hasattr(msg, 'type') and msg.type == 'ai':
                                accumulated_answer = msg.content
                                
                                # Extraire les sources des additional_kwargs
                                if hasattr(msg, 'additional_kwargs'):
                                    collected_sources = msg.additional_kwargs.get("sources", [])
                                
                                break
                        
                        # DÃ©tecter les tools utilisÃ©s dans la rÃ©ponse de l'agent pour Ã©mettre des status
                        vector_search_used = False
                        web_search_used = False
                        
                        # Analyser les sources pour dÃ©tecter les outils utilisÃ©s
                        for source in collected_sources:
                            source_type = source.get("type", "")
                            if source_type == "vector_search" or "similarity_score" in source:
                                if not vector_search_used:
                                    vector_search_used = True
                                    yield (
                                        json.dumps({
                                            "type": "status",
                                            "step": "vector_search",
                                            "message": "Recherche vectorielle en cours..."
                                        }) + "\n"
                                    )
                            elif source_type == "web_search" or "web" in source.get("url", "").lower():
                                if not web_search_used:
                                    web_search_used = True
                                    yield (
                                        json.dumps({
                                            "type": "status",
                                            "step": "web_search",
                                            "message": "Recherche web en cours..."
                                        }) + "\n"
                                    )
                        
                        # Ã‰mettre status pour la gÃ©nÃ©ration de la rÃ©ponse
                        yield (
                            json.dumps({
                                "type": "status",
                                "step": "generate",
                                "message": "GÃ©nÃ©ration de la rÃ©ponse..."
                            }) + "\n"
                        )
                        
                        # Streaming caractÃ¨re par caractÃ¨re pour une expÃ©rience fluide
                        for char in accumulated_answer:
                            yield (
                                json.dumps({
                                    "type": "message_chunk",
                                    "content": char,
                                    "node": "agent_rag"
                                }) + "\n"
                            )
                            # Petit dÃ©lai pour un streaming plus naturel
                            await asyncio.sleep(0.005)  # 5ms entre chaque caractÃ¨re
                        
                        yield (
                            json.dumps({
                                "type": "node_end",
                                "node": "agent_rag",
                                "message": f"RÃ©ponse gÃ©nÃ©rÃ©e ({len(accumulated_answer)} caractÃ¨res, {len(collected_sources)} sources)"
                            }) + "\n"
                        )
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # EVENT FINAL - Workflow complet
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            print(f"\n{'='*60}")
            print(f"Hybrid RAG Stream Completed")
            print(f"{'='*60}")
            print(f"RÃ©ponse: {len(accumulated_answer)} caractÃ¨res")
            print(f"Sources: {len(collected_sources)}")
            print(f"{'='*60}\n")
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # LOGGING DE LA CONVERSATION (PostgreSQL)
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            try:
                conn = psycopg2.connect(postgres_connection_string)
                cursor = conn.cursor()
                
                # DÃ©terminer les tools utilisÃ©s basÃ© sur les sources
                tools_used = []
                vector_searches = 0
                web_searches = 0
                
                for source in collected_sources:
                    source_type = source.get("type", "")
                    if source_type == "vector_search" or "similarity_score" in source:
                        if "vector_search" not in tools_used:
                            tools_used.append("vector_search")
                        vector_searches += 1
                    elif source_type == "web_search" or "web" in source.get("url", "").lower():
                        if "web_search" not in tools_used:
                            tools_used.append("web_search")
                        web_searches += 1
                
                if collected_sources and "reranker" not in tools_used:
                    tools_used.append("reranker")
                
                # InsÃ©rer dans la table conversations
                cursor.execute("""
                    INSERT INTO conversations (
                        id, question, answer, sources, tools_used,
                        vector_searches, web_searches, status
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (id) DO UPDATE SET
                        answer = EXCLUDED.answer,
                        sources = EXCLUDED.sources,
                        tools_used = EXCLUDED.tools_used,
                        vector_searches = EXCLUDED.vector_searches,
                        web_searches = EXCLUDED.web_searches,
                        status = EXCLUDED.status,
                        updated_at = NOW()
                """, (
                    thread_id,
                    body.question,
                    accumulated_answer,
                    json.dumps(collected_sources),
                    tools_used,
                    vector_searches,
                    web_searches,
                    "completed"
                ))
                
                conn.commit()
                cursor.close()
                conn.close()
                
                print(f"ğŸ’¾ Conversation {thread_id} enregistrÃ©e dans PostgreSQL")
                
            except Exception as log_error:
                print(f"âš ï¸ Erreur lors de l'enregistrement de la conversation: {str(log_error)}")
                # Ne pas bloquer le stream en cas d'erreur de logging
            
            yield (
                json.dumps({
                    "type": "complete",
                    "conversation_id": thread_id,
                    "question": body.question,
                    "answer": accumulated_answer,
                    "sources": collected_sources,
                    "metadata": {
                        "workflow": "hybrid_rag",
                        "sources_count": len(collected_sources),
                        "answer_length": len(accumulated_answer)
                    }
                }) + "\n"
            )
            
        except Exception as e:
            print(f"Erreur dans CRAG stream: {str(e)}")
            import traceback
            traceback.print_exc()
            
            # Logger l'erreur dans la base de donnÃ©es
            try:
                conn = psycopg2.connect(postgres_connection_string)
                cursor = conn.cursor()
                
                cursor.execute("""
                    INSERT INTO conversations (
                        id, question, status, error_message
                    ) VALUES (%s, %s, %s, %s)
                    ON CONFLICT (id) DO UPDATE SET
                        status = EXCLUDED.status,
                        error_message = EXCLUDED.error_message,
                        updated_at = NOW()
                """, (
                    thread_id,
                    body.question,
                    "error",
                    str(e)
                ))
                
                conn.commit()
                cursor.close()
                conn.close()
                
                print(f"ğŸ’¾ Erreur de conversation {thread_id} enregistrÃ©e dans PostgreSQL")
                
            except Exception as log_error:
                print(f"âš ï¸ Erreur lors de l'enregistrement de l'erreur: {str(log_error)}")
            
            yield (
                json.dumps({
                    "type": "error",
                    "error": str(e),
                    "message": f"Erreur: {str(e)}"
                }) + "\n"
            )
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # DÃ©sactive le buffering nginx
        }
    )


# ============================================================================
# NOTE : Endpoints /conversations/* et /sources/* SUPPRIMÃ‰S
# Pas de persistence Supabase - Seulement InMemorySaver pour mÃ©moire volatile
# ============================================================================


if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="127.0.0.1", port=8000, reload=True)